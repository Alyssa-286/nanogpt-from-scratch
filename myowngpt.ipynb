{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN9nNOJ11deP2m7sKychK3B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OmSCleNJFDAa","executionInfo":{"status":"ok","timestamp":1755799630477,"user_tz":-330,"elapsed":104,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"d95a4a54-dc09-4e9d-86df-56f662a91b96"},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: nvidia-smi: command not found\n","PyTorch: 2.8.0+cu126 CUDA: 12.6 GPU? False\n"]}],"source":["!nvidia-smi\n","import torch, sys, platform\n","print(\"PyTorch:\", torch.__version__, \"CUDA:\", torch.version.cuda, \"GPU?\", torch.cuda.is_available())\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"]},{"cell_type":"code","source":["!wget https://sherlock-holm.es/stories/plain-text/cnus.txt -O input.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkFaGuHqFG8q","executionInfo":{"status":"ok","timestamp":1755799632492,"user_tz":-330,"elapsed":2005,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"521ffc8e-71a8-4dc3-c38a-ec4b05e601cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-08-21 18:07:10--  https://sherlock-holm.es/stories/plain-text/cnus.txt\n","Resolving sherlock-holm.es (sherlock-holm.es)... 78.47.182.48, 157.90.249.21\n","Connecting to sherlock-holm.es (sherlock-holm.es)|78.47.182.48|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3382026 (3.2M) [text/plain]\n","Saving to: ‘input.txt’\n","\n","input.txt           100%[===================>]   3.22M  2.76MB/s    in 1.2s    \n","\n","2025-08-21 18:07:12 (2.76 MB/s) - ‘input.txt’ saved [3382026/3382026]\n","\n"]}]},{"cell_type":"code","source":["with open('input.txt', 'r', encoding='utf-8') as f:\n","    lines = f.readlines()\n","\n","# Identify and remove the specified lines\n","start_index = 0\n","end_index = 0\n","for i, line in enumerate(lines):\n","    if \"Arthur Conan Doyle\" in line:\n","        start_index = i\n","    if \"The Sign of the Four\" in line:\n","        end_index = i\n","        break\n","\n","# Keep the remaining lines and remove excessive blank lines\n","processed_lines = []\n","for line in lines[:start_index] + lines[end_index+1:]:\n","    if line.strip() or (processed_lines and processed_lines[-1].strip()):\n","        processed_lines.append(line)\n","\n","text = ''.join(processed_lines)\n","\n","print(\"Removed lines:\")\n","print(''.join(lines[start_index : end_index + 1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RDlZUre0MJ-0","executionInfo":{"status":"ok","timestamp":1755799632539,"user_tz":-330,"elapsed":44,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"773551e5-17e2-4094-ee67-83ea06c346c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Removed lines:\n","                               Arthur Conan Doyle\n","\n","\n","\n","                                Table of contents\n","\n","               A Study In Scarlet\n","\n","               The Sign of the Four\n","\n"]}]},{"cell_type":"code","source":["print(\"length of dataset in characters: \", len(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ChbKlKd_MsNl","executionInfo":{"status":"ok","timestamp":1755799632588,"user_tz":-330,"elapsed":46,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"7974d691-f73d-4a15-dc42-e01b718ca55f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["length of dataset in characters:  3380975\n"]}]},{"cell_type":"code","source":["print(text[:11000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"h_5jvYl7MwhT","executionInfo":{"status":"ok","timestamp":1755799632610,"user_tz":-330,"elapsed":20,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"8cfa613e-5cbe-4b7d-d8e4-b0ab02370cb9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                          THE COMPLETE SHERLOCK HOLMES\n","\n","                  The Adventures of Sherlock Holmes\n","               A Scandal in Bohemia\n","               The Red-Headed League\n","               A Case of Identity\n","               The Boscombe Valley Mystery\n","               The Five Orange Pips\n","               The Man with the Twisted Lip\n","               The Adventure of the Blue Carbuncle\n","               The Adventure of the Speckled Band\n","               The Adventure of the Engineer's Thumb\n","               The Adventure of the Noble Bachelor\n","               The Adventure of the Beryl Coronet\n","               The Adventure of the Copper Beeches\n","\n","                  The Memoirs of Sherlock Holmes\n","               Silver Blaze\n","               The Yellow Face\n","               The Stock-Broker's Clerk\n","               The \"Gloria Scott\"\n","               The Musgrave Ritual\n","               The Reigate Squires\n","               The Crooked Man\n","               The Resident Patient\n","               The Greek Interpreter\n","               The Naval Treaty\n","               The Final Problem\n","\n","                  The Return of Sherlock Holmes\n","               The Adventure of the Empty House\n","               The Adventure of the Norwood Builder\n","               The Adventure of the Dancing Men\n","               The Adventure of the Solitary Cyclist\n","               The Adventure of the Priory School\n","               The Adventure of Black Peter\n","               The Adventure of Charles Augustus Milverton\n","               The Adventure of the Six Napoleons\n","               The Adventure of the Three Students\n","               The Adventure of the Golden Pince-Nez\n","               The Adventure of the Missing Three-Quarter\n","               The Adventure of the Abbey Grange\n","               The Adventure of the Second Stain\n","\n","               The Hound of the Baskervilles\n","\n","               The Valley Of Fear\n","\n","                  His Last Bow\n","               Preface\n","               The Adventure of Wisteria Lodge\n","               The Adventure of the Cardboard Box\n","               The Adventure of the Red Circle\n","               The Adventure of the Bruce-Partington Plans\n","               The Adventure of the Dying Detective\n","               The Disappearance of Lady Frances Carfax\n","               The Adventure of the Devil's Foot\n","               His Last Bow\n","\n","                               A STUDY IN SCARLET\n","\n","                                Table of contents\n","\n","         Part I\n","        Mr. Sherlock Holmes\n","        The Science Of Deduction\n","        The Lauriston Garden Mystery\n","        What John Rance Had To Tell\n","        Our Advertisement Brings A Visitor\n","        Tobias Gregson Shows What He Can Do\n","        Light In The Darkness\n","\n","         Part II\n","        On The Great Alkali Plain\n","        The Flower Of Utah\n","        John Ferrier Talks With The Prophet\n","        A Flight For Life\n","        The Avenging Angels\n","        A Continuation Of The Reminiscences Of John Watson, M.D.\n","        The Conclusion\n","\n","                                      PART I\n","\n","                   (Being a reprint from the reminiscences of\n","                              John H. Watson, M.D.,\n","                      late of the Army Medical Department.)\n","\n","          CHAPTER I\n","          Mr. Sherlock Holmes\n","\n","     In the year 1878 I took my degree of Doctor of Medicine of the\n","     University of London, and proceeded to Netley to go through the\n","     course prescribed for surgeons in the army. Having completed my\n","     studies there, I was duly attached to the Fifth Northumberland\n","     Fusiliers as Assistant Surgeon. The regiment was stationed in India\n","     at the time, and before I could join it, the second Afghan war had\n","     broken out. On landing at Bombay, I learned that my corps had\n","     advanced through the passes, and was already deep in the enemy's\n","     country. I followed, however, with many other officers who were in\n","     the same situation as myself, and succeeded in reaching Candahar in\n","     safety, where I found my regiment, and at once entered upon my new\n","     duties.\n","\n","     The campaign brought honours and promotion to many, but for me it had\n","     nothing but misfortune and disaster. I was removed from my brigade\n","     and attached to the Berkshires, with whom I served at the fatal\n","     battle of Maiwand. There I was struck on the shoulder by a Jezail\n","     bullet, which shattered the bone and grazed the subclavian artery. I\n","     should have fallen into the hands of the murderous Ghazis had it not\n","     been for the devotion and courage shown by Murray, my orderly, who\n","     threw me across a pack-horse, and succeeded in bringing me safely to\n","     the British lines.\n","\n","     Worn with pain, and weak from the prolonged hardships which I had\n","     undergone, I was removed, with a great train of wounded sufferers, to\n","     the base hospital at Peshawar. Here I rallied, and had already\n","     improved so far as to be able to walk about the wards, and even to\n","     bask a little upon the verandah, when I was struck down by enteric\n","     fever, that curse of our Indian possessions. For months my life was\n","     despaired of, and when at last I came to myself and became\n","     convalescent, I was so weak and emaciated that a medical board\n","     determined that not a day should be lost in sending me back to\n","     England. I was dispatched, accordingly, in the troopship Orontes, and\n","     landed a month later on Portsmouth jetty, with my health\n","     irretrievably ruined, but with permission from a paternal government\n","     to spend the next nine months in attempting to improve it.\n","\n","     I had neither kith nor kin in England, and was therefore as free as\n","     air--or as free as an income of eleven shillings and sixpence a day\n","     will permit a man to be. Under such circumstances, I naturally\n","     gravitated to London, that great cesspool into which all the loungers\n","     and idlers of the Empire are irresistibly drained. There I stayed for\n","     some time at a private hotel in the Strand, leading a comfortless,\n","     meaningless existence, and spending such money as I had, considerably\n","     more freely than I ought. So alarming did the state of my finances\n","     become, that I soon realized that I must either leave the metropolis\n","     and rusticate somewhere in the country, or that I must make a\n","     complete alteration in my style of living. Choosing the latter\n","     alternative, I began by making up my mind to leave the hotel, and to\n","     take up my quarters in some less pretentious and less expensive\n","     domicile.\n","\n","     On the very day that I had come to this conclusion, I was standing at\n","     the Criterion Bar, when some one tapped me on the shoulder, and\n","     turning round I recognized young Stamford, who had been a dresser\n","     under me at Bart's. The sight of a friendly face in the great\n","     wilderness of London is a pleasant thing indeed to a lonely man. In\n","     old days Stamford had never been a particular crony of mine, but now\n","     I hailed him with enthusiasm, and he, in his turn, appeared to be\n","     delighted to see me. In the exuberance of my joy, I asked him to\n","     lunch with me at the Holborn, and we started off together in a\n","     hansom.\n","\n","     \"Whatever have you been doing with yourself, Watson?\" he asked in\n","     undisguised wonder, as we rattled through the crowded London streets.\n","     \"You are as thin as a lath and as brown as a nut.\"\n","\n","     I gave him a short sketch of my adventures, and had hardly concluded\n","     it by the time that we reached our destination.\n","\n","     \"Poor devil!\" he said, commiseratingly, after he had listened to my\n","     misfortunes. \"What are you up to now?\"\n","\n","     \"Looking for lodgings,\" I answered. \"Trying to solve the problem as\n","     to whether it is possible to get comfortable rooms at a reasonable\n","     price.\"\n","\n","     \"That's a strange thing,\" remarked my companion; \"you are the second\n","     man to-day that has used that expression to me.\"\n","\n","     \"And who was the first?\" I asked.\n","\n","     \"A fellow who is working at the chemical laboratory up at the\n","     hospital. He was bemoaning himself this morning because he could not\n","     get someone to go halves with him in some nice rooms which he had\n","     found, and which were too much for his purse.\"\n","\n","     \"By Jove!\" I cried, \"if he really wants someone to share the rooms\n","     and the expense, I am the very man for him. I should prefer having a\n","     partner to being alone.\"\n","\n","     Young Stamford looked rather strangely at me over his wine-glass.\n","     \"You don't know Sherlock Holmes yet,\" he said; \"perhaps you would not\n","     care for him as a constant companion.\"\n","\n","     \"Why, what is there against him?\"\n","\n","     \"Oh, I didn't say there was anything against him. He is a little\n","     queer in his ideas--an enthusiast in some branches of science. As far\n","     as I know he is a decent fellow enough.\"\n","\n","     \"A medical student, I suppose?\" said I.\n","\n","     \"No--I have no idea what he intends to go in for. I believe he is\n","     well up in anatomy, and he is a first-class chemist; but, as far as I\n","     know, he has never taken out any systematic medical classes. His\n","     studies are very desultory and eccentric, but he has amassed a lot of\n","     out-of-the way knowledge which would astonish his professors.\"\n","\n","     \"Did you never ask him what he was going in for?\" I asked.\n","\n","     \"No; he is not a man that it is easy to draw out, though he can be\n","     communicative enough when the fancy seizes him.\"\n","\n","     \"I should like to meet him,\" I said. \"If I am to lodge with anyone, I\n","     should prefer a man of studious and quiet habits. I am not strong\n","     enough yet to stand much noise or excitement. I had enough of both in\n","     Afghanistan to last me for the remainder of my natural existence. How\n","     could I meet this friend of yours?\"\n","\n","     \"He is sure to be at the laboratory,\" returned my companion. \"He\n","     either avoids the place for weeks, or else he works there from\n","     morning to night. If you like, we shall drive round together after\n","     luncheon.\"\n","\n","     \"Certainly,\" I answered, and the conversation drifted away into other\n","     channels.\n","\n","     As we made our way to the hospital after leaving the Holborn,\n","     Stamford gave me a few more particulars about the gentleman whom I\n","     proposed to take as a fellow-lodger.\n","\n","     \"You mustn't blame me if you don't get on with him,\" he said; \"I know\n","     nothing more of him than I have learned from meeting him occasionally\n","     in the laboratory. You proposed this arrangement, so you must not\n","     hold me responsible.\"\n","\n","     \"If we don't get on it will be easy to part company,\" I answered. \"It\n","     seems to me, Stamford,\" I added, looking hard at my companion, \"that\n","     you have some reason for washing your hands of the matter. Is this\n","     fellow's temper so formidable, or what is it? Don't be mealy-mouthed\n","     about it.\"\n","\n","     \"It is not easy to express the inexpressible,\" he answered with a\n","     laugh. \"Holmes is a little too scientific for my tastes--it\n","     approaches to cold-bloodedness. I could imagine his giving a friend a\n","     little pinch of the latest vege\n"]}]},{"cell_type":"code","source":["chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","print(''.join(chars))\n","print(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WOBRNrg-M6DF","executionInfo":{"status":"ok","timestamp":1755799632674,"user_tz":-330,"elapsed":58,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"cbe48984-fa5e-4220-e1bb-18c6a45bfa85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," !\"&'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]`abcdefghijklmnopqrstuvwxyz£°½ßàâèéêîñôöûü’\n","97\n"]}]},{"cell_type":"code","source":["stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","print(encode(\"hii there\"))\n","print(decode(encode(\"hii there\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uzf7crOaNQjY","executionInfo":{"status":"ok","timestamp":1755799632716,"user_tz":-330,"elapsed":38,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"9c6d017a-3a94-4c23-bc04-1671a4f31215"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[62, 63, 63, 1, 74, 62, 59, 72, 59]\n","hii there\n"]}]},{"cell_type":"code","source":["import torch # we use PyTorch: https://pytorch.org\n","data = torch.tensor(encode(text), dtype=torch.long)\n","print(data.shape, data.dtype)\n","print(data[:1000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ahMi9UUO7bR","executionInfo":{"status":"ok","timestamp":1755799633574,"user_tz":-330,"elapsed":839,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"93dc04ed-2da9-4f8b-f0c5-8c878f6d34d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3380975]) torch.int64\n","tensor([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n","         1,  1,  1,  1,  1,  1,  1,  1, 45, 33, 30,  1, 28, 40, 38, 41, 37, 30,\n","        45, 30,  1, 44, 33, 30, 43, 37, 40, 28, 36,  1, 33, 40, 37, 38, 30, 44,\n","         0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n","         1,  1, 45, 62, 59,  1, 26, 58, 76, 59, 68, 74, 75, 72, 59, 73,  1, 69,\n","        60,  1, 44, 62, 59, 72, 66, 69, 57, 65,  1, 33, 69, 66, 67, 59, 73,  0,\n","         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 26,  1, 44,\n","        57, 55, 68, 58, 55, 66,  1, 63, 68,  1, 27, 69, 62, 59, 67, 63, 55,  0,\n","         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 45, 62, 59,\n","         1, 43, 59, 58, 10, 33, 59, 55, 58, 59, 58,  1, 37, 59, 55, 61, 75, 59,\n","         0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 26,  1,\n","        28, 55, 73, 59,  1, 69, 60,  1, 34, 58, 59, 68, 74, 63, 74, 79,  0,  1,\n","         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 45, 62, 59,  1,\n","        27, 69, 73, 57, 69, 67, 56, 59,  1, 47, 55, 66, 66, 59, 79,  1, 38, 79,\n","        73, 74, 59, 72, 79,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n","         1,  1,  1, 45, 62, 59,  1, 31, 63, 76, 59,  1, 40, 72, 55, 68, 61, 59,\n","         1, 41, 63, 70, 73,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n","         1,  1,  1, 45, 62, 59,  1, 38, 55, 68,  1, 77, 63, 74, 62,  1, 74, 62,\n","        59,  1, 45, 77, 63, 73, 74, 59, 58,  1, 37, 63, 70,  0,  1,  1,  1,  1,\n","         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 45, 62, 59,  1, 26, 58, 76,\n","        59, 68, 74, 75, 72, 59,  1, 69, 60,  1, 74, 62, 59,  1, 27, 66, 75, 59,\n","         1, 28, 55, 72, 56, 75, 68, 57, 66, 59,  0,  1,  1,  1,  1,  1,  1,  1,\n","         1,  1,  1,  1,  1,  1,  1,  1, 45, 62, 59,  1, 26, 58, 76, 59, 68, 74,\n","        75, 72, 59,  1, 69, 60,  1, 74, 62, 59,  1, 44, 70, 59, 57, 65, 66, 59,\n","        58,  1, 27, 55, 68, 58,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n","         1,  1,  1,  1, 45, 62, 59,  1, 26, 58, 76, 59, 68, 74, 75, 72, 59,  1,\n","        69, 60,  1, 74, 62, 59,  1, 30, 68, 61, 63, 68, 59, 59, 72,  5, 73,  1,\n","        45, 62, 75, 67, 56,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n","         1,  1,  1, 45, 62, 59,  1, 26, 58, 76, 59, 68, 74, 75, 72, 59,  1, 69,\n","        60,  1, 74, 62, 59,  1, 39, 69, 56, 66, 59,  1, 27, 55, 57, 62, 59, 66,\n","        69, 72,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n","        45, 62, 59,  1, 26, 58, 76, 59, 68, 74, 75, 72, 59,  1, 69, 60,  1, 74,\n","        62, 59,  1, 27, 59, 72, 79, 66,  1, 28, 69, 72, 69, 68, 59, 74,  0,  1,\n","         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 45, 62, 59,  1,\n","        26, 58, 76, 59, 68, 74, 75, 72, 59,  1, 69, 60,  1, 74, 62, 59,  1, 28,\n","        69, 70, 70, 59, 72,  1, 27, 59, 59, 57, 62, 59, 73,  0,  0,  1,  1,  1,\n","         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 45, 62, 59,\n","         1, 38, 59, 67, 69, 63, 72, 73,  1, 69, 60,  1, 44, 62, 59, 72, 66, 69,\n","        57, 65,  1, 33, 69, 66, 67, 59, 73,  0,  1,  1,  1,  1,  1,  1,  1,  1,\n","         1,  1,  1,  1,  1,  1,  1, 44, 63, 66, 76, 59, 72,  1, 27, 66, 55, 80,\n","        59,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 45,\n","        62, 59,  1, 50, 59, 66, 66, 69, 77,  1, 31, 55, 57, 59,  0,  1,  1,  1,\n","         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 45, 62, 59,  1, 44, 74,\n","        69, 57, 65, 10, 27, 72, 69, 65, 59, 72,  5, 73,  1, 28, 66, 59, 72, 65,\n","         0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 45, 62,\n","        59,  1,  3, 32, 66, 69, 72, 63, 55,  1, 44, 57, 69, 74, 74,  3,  0,  1,\n","         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 45, 62, 59,  1,\n","        38, 75, 73, 61, 72, 55, 76, 59,  1, 43, 63, 74, 75, 55, 66,  0,  1,  1,\n","         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 45, 62, 59,  1, 43,\n","        59, 63, 61, 55, 74, 59,  1, 44, 71, 75, 63, 72, 59, 73,  0,  1,  1,  1,\n","         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 45, 62, 59,  1, 28, 72,\n","        69, 69, 65, 59, 58,  1, 38, 55, 68,  0,  1,  1,  1,  1,  1,  1,  1,  1,\n","         1,  1,  1,  1,  1,  1,  1, 45, 62, 59,  1, 43, 59, 73, 63, 58, 59, 68,\n","        74,  1, 41, 55, 74, 63, 59, 68, 74,  0,  1,  1,  1,  1,  1,  1,  1,  1,\n","         1,  1,  1,  1,  1,  1,  1, 45, 62, 59,  1, 32, 72, 59, 59, 65,  1, 34,\n","        68, 74, 59, 72, 70, 72, 59, 74, 59, 72])\n"]}]},{"cell_type":"code","source":["n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]"],"metadata":{"id":"KWYmuth5PqWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["block_size = 8\n","train_data[:block_size+1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2xQmN7w2QCBc","executionInfo":{"status":"ok","timestamp":1755799633593,"user_tz":-330,"elapsed":12,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"60fbc7de-3cf3-44fc-f74e-39f084c0f626"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])"]},"metadata":{},"execution_count":163}]},{"cell_type":"code","source":["x = train_data[:block_size]\n","y = train_data[1:block_size+1]\n","for t in range(block_size):\n","    context = x[:t+1]\n","    target = y[t]\n","    print(f\"when input is {context} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hdr-OQf1QFJ1","executionInfo":{"status":"ok","timestamp":1755799633612,"user_tz":-330,"elapsed":15,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"59f88475-d9e4-4a28-e1c1-78adbe693bf4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["when input is tensor([1]) the target: 1\n","when input is tensor([1, 1]) the target: 1\n","when input is tensor([1, 1, 1]) the target: 1\n","when input is tensor([1, 1, 1, 1]) the target: 1\n","when input is tensor([1, 1, 1, 1, 1]) the target: 1\n","when input is tensor([1, 1, 1, 1, 1, 1]) the target: 1\n","when input is tensor([1, 1, 1, 1, 1, 1, 1]) the target: 1\n","when input is tensor([1, 1, 1, 1, 1, 1, 1, 1]) the target: 1\n"]}]},{"cell_type":"code","source":["# @title\n","torch.manual_seed(1337)\n","batch_size = 4 # how many independent sequences will we process in parallel?\n","block_size = 8 # what is the maximum context length for predictions?\n","\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    return x, y\n","\n","xb, yb = get_batch('train')\n","print('inputs:')\n","print(xb.shape)\n","print(xb)\n","print('targets:')\n","print(yb.shape)\n","print(yb)\n","\n","print('----')\n","\n","for b in range(batch_size): # batch dimension\n","    for t in range(block_size): # time dimension\n","        context = xb[b, :t+1]\n","        target = yb[b,t]\n","        print(f\"when input is {context.tolist()} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Y80__ydQHyw","executionInfo":{"status":"ok","timestamp":1755799633688,"user_tz":-330,"elapsed":73,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"b2ca61ac-bb58-449e-bebc-6e1441a7b361"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs:\n","torch.Size([4, 8])\n","tensor([[ 1, 57, 69, 68, 64, 59, 57, 74],\n","        [73, 74, 72, 59, 59, 74,  0,  1],\n","        [69,  1, 74, 62, 59,  1, 66, 55],\n","        [58,  9,  3,  1, 55, 73, 65, 59]])\n","targets:\n","torch.Size([4, 8])\n","tensor([[57, 69, 68, 64, 59, 57, 74, 75],\n","        [74, 72, 59, 59, 74,  0,  1,  1],\n","        [ 1, 74, 62, 59,  1, 66, 55, 73],\n","        [ 9,  3,  1, 55, 73, 65, 59, 58]])\n","----\n","when input is [1] the target: 57\n","when input is [1, 57] the target: 69\n","when input is [1, 57, 69] the target: 68\n","when input is [1, 57, 69, 68] the target: 64\n","when input is [1, 57, 69, 68, 64] the target: 59\n","when input is [1, 57, 69, 68, 64, 59] the target: 57\n","when input is [1, 57, 69, 68, 64, 59, 57] the target: 74\n","when input is [1, 57, 69, 68, 64, 59, 57, 74] the target: 75\n","when input is [73] the target: 74\n","when input is [73, 74] the target: 72\n","when input is [73, 74, 72] the target: 59\n","when input is [73, 74, 72, 59] the target: 59\n","when input is [73, 74, 72, 59, 59] the target: 74\n","when input is [73, 74, 72, 59, 59, 74] the target: 0\n","when input is [73, 74, 72, 59, 59, 74, 0] the target: 1\n","when input is [73, 74, 72, 59, 59, 74, 0, 1] the target: 1\n","when input is [69] the target: 1\n","when input is [69, 1] the target: 74\n","when input is [69, 1, 74] the target: 62\n","when input is [69, 1, 74, 62] the target: 59\n","when input is [69, 1, 74, 62, 59] the target: 1\n","when input is [69, 1, 74, 62, 59, 1] the target: 66\n","when input is [69, 1, 74, 62, 59, 1, 66] the target: 55\n","when input is [69, 1, 74, 62, 59, 1, 66, 55] the target: 73\n","when input is [58] the target: 9\n","when input is [58, 9] the target: 3\n","when input is [58, 9, 3] the target: 1\n","when input is [58, 9, 3, 1] the target: 55\n","when input is [58, 9, 3, 1, 55] the target: 73\n","when input is [58, 9, 3, 1, 55, 73] the target: 65\n","when input is [58, 9, 3, 1, 55, 73, 65] the target: 59\n","when input is [58, 9, 3, 1, 55, 73, 65, 59] the target: 58\n"]}]},{"cell_type":"code","source":["print(xb) # our input to the transformer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6QQ4ot9CQL3z","executionInfo":{"status":"ok","timestamp":1755799633701,"user_tz":-330,"elapsed":11,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"35999b0f-a7e9-497a-d17c-2660adfbbef6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1, 57, 69, 68, 64, 59, 57, 74],\n","        [73, 74, 72, 59, 59, 74,  0,  1],\n","        [69,  1, 74, 62, 59,  1, 66, 55],\n","        [58,  9,  3,  1, 55, 73, 65, 59]])\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","torch.manual_seed(1337)\n","\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        logits = self.token_embedding_table(idx) # (B,T,C)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # get the predictions\n","            logits, loss = self(idx)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n","\n","m = BigramLanguageModel(vocab_size)\n","logits, loss = m(xb, yb)\n","print(logits.shape)\n","print(loss)\n","\n","print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FjP_GloSQPbb","executionInfo":{"status":"ok","timestamp":1755799633711,"user_tz":-330,"elapsed":8,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"cf8ab298-3d8d-4be3-e2e2-6265586b0f85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 97])\n","tensor(5.0090, grad_fn=<NllLossBackward0>)\n","\n","E&h-îBe.dàgOéètöVBDwOm5PgOv£0;DIF-(½]uu;t\"RZIV£îMK.-tjAbsCü’86Oa'WCFEjâg9pDb)05WwT8r\"\n","WH?PôSpmb’&\n","LA\n"]}]},{"cell_type":"code","source":["# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"],"metadata":{"id":"MeaHD1H4QTPJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 128\n","for steps in range(24000): # increase number of steps for good results...\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = m(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","print(loss.item())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hng3jdiEQcM4","executionInfo":{"status":"ok","timestamp":1755799785370,"user_tz":-330,"elapsed":151651,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"cc94bd2c-0f81-4b15-b46a-b6c555c04def"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.3997576236724854\n"]}]},{"cell_type":"code","source":["print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=900)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aCBKi0d_QgCb","executionInfo":{"status":"ok","timestamp":1755799785467,"user_tz":-330,"elapsed":129,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"39ac03fe-1259-42f3-a210-e1e3bdbed290"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n"," t  tulg  bupus, mes heselaso beasasenthag. bunt s t tis \"\n","\n"," lereroont's\n"," ad ounct \" po oray inibe leas ff m er-arenthe  pas y.  sit? \"Mrs arouled abe-\n"," m.  adidea he. achad aiowhes his  Sonda d it.\"\n"," tespims iorsorir ththofofor orthe plllut mon  s  cupetiltintenthabofenininde m of owanc,\"be  wee,\"Incan erorily alurde ap-nthevee,  sik athee he\n","   Dore?\" oked an ses and.\"Soth ctheryouken\n","   mso w\n","  f  asif mofo An He Itwice o  nil shinergicur\n","\n","  thes.' bolild.  h t  s  orkerca\n"," t  t as s\n"," idooter asichanisle wivin\n"," win'ss o bthize jerurma ad d hoknd teto ld bespr t f t.\"\n"," \"Hea Mrisofotrat ghofothen ivechens llang I thenereck foture f d. avaforespus tsespoftoucoputhturay  o  chavedrafrinld pr wily wrsh wint, hand      ony  wedouboun   bithe\n"," whese orucesttoke t. mabad wan m Weneavickn uth g tt t outhedurinether. st w was s nind.\"\n","  n  w-\"  s  he\n"," bl elfostind   catoftht q ss  my of timing\n"]}]},{"cell_type":"code","source":["# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n","torch.manual_seed(42)\n","a = torch.tril(torch.ones(3, 3))\n","a = a / torch.sum(a, 1, keepdim=True)\n","b = torch.randint(0,10,(3,2)).float()\n","c = a @ b\n","print('a=')\n","print(a)\n","print('--')\n","print('b=')\n","print(b)\n","print('--')\n","print('c=')\n","print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F-eI7YupQivt","executionInfo":{"status":"ok","timestamp":1755799785484,"user_tz":-330,"elapsed":16,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"5720ea9e-c646-45cf-bb50-1daf6b23d673"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a=\n","tensor([[1.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000],\n","        [0.3333, 0.3333, 0.3333]])\n","--\n","b=\n","tensor([[2., 7.],\n","        [6., 4.],\n","        [6., 5.]])\n","--\n","c=\n","tensor([[2.0000, 7.0000],\n","        [4.0000, 5.5000],\n","        [4.6667, 5.3333]])\n"]}]},{"cell_type":"code","source":["# consider the following toy example:\n","\n","torch.manual_seed(1337)\n","B,T,C = 4,8,2 # batch, time, channels\n","x = torch.randn(B,T,C)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ODLmn8lyQlyB","executionInfo":{"status":"ok","timestamp":1755799785508,"user_tz":-330,"elapsed":20,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"cb2c6f2c-2f3a-4ca9-a911-2459a3aade12"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 2])"]},"metadata":{},"execution_count":172}]},{"cell_type":"code","source":["# We want x[b,t] = mean_{i<=t} x[b,i]\n","xbow = torch.zeros((B,T,C))\n","for b in range(B):\n","    for t in range(T):\n","        xprev = x[b,:t+1] # (t,C)\n","        xbow[b,t] = torch.mean(xprev, 0)\n"],"metadata":{"id":"wt4YIFZlQolp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# version 2: using matrix multiply for a weighted aggregation\n","wei = torch.tril(torch.ones(T, T))\n","wei = wei / wei.sum(1, keepdim=True)\n","xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n","torch.allclose(xbow, xbow2, atol=1e-6)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7v-Y-oswQquV","executionInfo":{"status":"ok","timestamp":1755799785551,"user_tz":-330,"elapsed":28,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"02641dd1-0a39-42eb-ac16-cbd039d00cc9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":174}]},{"cell_type":"code","source":["# version 3: use Softmax\n","tril = torch.tril(torch.ones(T, T))\n","wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","wei = F.softmax(wei, dim=-1)\n","xbow3 = wei @ x\n","torch.allclose(xbow, xbow3, atol=1e-6)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VkTKga0cQs88","executionInfo":{"status":"ok","timestamp":1755799785568,"user_tz":-330,"elapsed":19,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"8a7f7b7f-080d-4551-9d30-2baeaac63d67"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":175}]},{"cell_type":"code","source":["# version 4: self-attention!\n","torch.manual_seed(1337)\n","B,T,C = 4,8,32 # batch, time, channels\n","x = torch.randn(B,T,C)\n","\n","# let's see a single Head perform self-attention\n","head_size = 16\n","key = nn.Linear(C, head_size, bias=False)\n","query = nn.Linear(C, head_size, bias=False)\n","value = nn.Linear(C, head_size, bias=False)\n","k = key(x)   # (B, T, 16)\n","q = query(x) # (B, T, 16)\n","wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n","\n","tril = torch.tril(torch.ones(T, T))\n","#wei = torch.zeros((T,T))\n","wei = wei.masked_fill(tril == 0, float('-inf'))\n","wei = F.softmax(wei, dim=-1)\n","\n","v = value(x)\n","out = wei @ v\n","#out = wei @ x\n","\n","out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"14YEdNHaQx2A","executionInfo":{"status":"ok","timestamp":1755799785581,"user_tz":-330,"elapsed":11,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"b1c66408-3d2a-4e4e-af95-89dff418e1a5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 16])"]},"metadata":{},"execution_count":176}]},{"cell_type":"code","source":["wei[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r2ckoAtpQ05x","executionInfo":{"status":"ok","timestamp":1755799785596,"user_tz":-330,"elapsed":11,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"9c42654b-b26d-47e5-e4e4-3dc7e82dd142"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n","        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n","        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n","        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n","       grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":177}]},{"cell_type":"code","source":["k = torch.randn(B,T,head_size)\n","q = torch.randn(B,T,head_size)\n","wei = q @ k.transpose(-2, -1) * head_size**-0.5"],"metadata":{"id":"8JHSLfTPQ3j7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["k.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nH367G7xQ-nk","executionInfo":{"status":"ok","timestamp":1755799785621,"user_tz":-330,"elapsed":11,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"2ebeb67b-a029-445a-bed8-c9c32831af7d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0449)"]},"metadata":{},"execution_count":179}]},{"cell_type":"code","source":["q.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YK4vyRuzRBPF","executionInfo":{"status":"ok","timestamp":1755799785638,"user_tz":-330,"elapsed":12,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"0320a031-b2c4-47f8-f8eb-8610c6baca0c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0700)"]},"metadata":{},"execution_count":180}]},{"cell_type":"code","source":["wei.var()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ySGPfwXqRDvl","executionInfo":{"status":"ok","timestamp":1755799785655,"user_tz":-330,"elapsed":13,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"cd8155a4-7e3b-4400-e4f3-528c4c197662"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.0918)"]},"metadata":{},"execution_count":181}]},{"cell_type":"code","source":["torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMdDJ7HxRHEf","executionInfo":{"status":"ok","timestamp":1755799785677,"user_tz":-330,"elapsed":19,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"34f933dd-4087-42c6-a1b4-2d3eac30b25b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"]},"metadata":{},"execution_count":182}]},{"cell_type":"code","source":["torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S2Pk_LuDRJum","executionInfo":{"status":"ok","timestamp":1755799785757,"user_tz":-330,"elapsed":72,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"1a61de17-268e-44bb-a6af-ca67f5606a22"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"]},"metadata":{},"execution_count":183}]},{"cell_type":"code","source":["class LayerNorm1d: # (used to be BatchNorm1d)\n","\n","  def __init__(self, dim, eps=1e-5, momentum=0.1):\n","    self.eps = eps\n","    self.gamma = torch.ones(dim)\n","    self.beta = torch.zeros(dim)\n","\n","  def __call__(self, x):\n","    # calculate the forward pass\n","    xmean = x.mean(1, keepdim=True) # batch mean\n","    xvar = x.var(1, keepdim=True) # batch variance\n","    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n","    self.out = self.gamma * xhat + self.beta\n","    return self.out\n","\n","  def parameters(self):\n","    return [self.gamma, self.beta]\n","\n","torch.manual_seed(1337)\n","module = LayerNorm1d(100)\n","x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n","x = module(x)\n","x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B1NF3c4WRL8d","executionInfo":{"status":"ok","timestamp":1755799785760,"user_tz":-330,"elapsed":59,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"b138f4a5-79bb-4a27-c1e5-25a38cac3fd5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 100])"]},"metadata":{},"execution_count":184}]},{"cell_type":"code","source":["x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TmFAKgEFROYr","executionInfo":{"status":"ok","timestamp":1755799785761,"user_tz":-330,"elapsed":46,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"fb799da5-d872-4e70-e791-3b0501b7ebb1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.1469), tensor(0.8803))"]},"metadata":{},"execution_count":185}]},{"cell_type":"code","source":["x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcmakNypRRXR","executionInfo":{"status":"ok","timestamp":1755799785762,"user_tz":-330,"elapsed":30,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"6de18d75-d9f2-43a7-ef8a-71281f31409f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(-9.5367e-09), tensor(1.0000))"]},"metadata":{},"execution_count":186}]},{"cell_type":"code","source":["# French to English translation example:\n","\n","# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n","# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n","\n"],"metadata":{"id":"CxHOECpWRVfr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","# hyperparameters\n","batch_size = 16 # how many independent sequences will we process in parallel?\n","block_size = 32 # what is the maximum context length for predictions?\n","max_iters = 5000\n","eval_interval = 100\n","learning_rate = 1e-3\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embd = 64\n","n_head = 4\n","n_layer = 4\n","dropout = 0.0\n","# ------------\n","\n","torch.manual_seed(1337)\n","\n","# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","with open('input.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","\n","# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n","\n","# Train and test splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# data loading\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size, bias=False)\n","        self.query = nn.Linear(n_embd, head_size, bias=False)\n","        self.value = nn.Linear(n_embd, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B,T,C = x.shape\n","        k = self.key(x)   # (B,T,C)\n","        q = self.query(x) # (B,T,C)\n","        # compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        wei = F.softmax(wei, dim=-1) # (B, T, T)\n","        wei = self.dropout(wei)\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,C)\n","        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self-attention in parallel \"\"\"\n","\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embd, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)\n","        out = self.dropout(self.proj(out))\n","        return out\n","\n","class FeedFoward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embd, 4 * n_embd),\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embd, n_embd),\n","            nn.Dropout(dropout),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","\n","    def __init__(self, n_embd, n_head):\n","        # n_embd: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        self.ffwd = FeedFoward(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.sa(self.ln1(x))\n","        x = x + self.ffwd(self.ln2(x))\n","        return x\n","\n","# super simple bigram model\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n","        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size)\n","\n","    def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n","        x = tok_emb + pos_emb # (B,T,C)\n","        x = self.blocks(x) # (B,T,C)\n","        x = self.ln_f(x) # (B,T,C)\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx\n","\n","model = BigramLanguageModel()\n","m = model.to(device)\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for iter in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0 or iter == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","# generate from the model\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_FGUHB82Ra_2","executionInfo":{"status":"ok","timestamp":1755800303927,"user_tz":-330,"elapsed":518172,"user":{"displayName":"Alyssa L","userId":"00970608806885122398"}},"outputId":"cc6057a2-99d4-4093-c2db-1b8686ba0f44"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.213857 M parameters\n","step 0: train loss 4.7080, val loss 4.7127\n","step 100: train loss 2.5030, val loss 2.5195\n","step 200: train loss 2.3202, val loss 2.3372\n","step 300: train loss 2.2387, val loss 2.2714\n","step 400: train loss 2.1936, val loss 2.2013\n","step 500: train loss 2.1334, val loss 2.1548\n","step 600: train loss 2.0818, val loss 2.1006\n","step 700: train loss 2.0271, val loss 2.0395\n","step 800: train loss 1.9876, val loss 2.0114\n","step 900: train loss 1.9517, val loss 1.9796\n","step 1000: train loss 1.9308, val loss 1.9415\n","step 1100: train loss 1.9022, val loss 1.9171\n","step 1200: train loss 1.8798, val loss 1.8751\n","step 1300: train loss 1.8485, val loss 1.8746\n","step 1400: train loss 1.8247, val loss 1.8532\n","step 1500: train loss 1.8170, val loss 1.8416\n","step 1600: train loss 1.7897, val loss 1.8235\n","step 1700: train loss 1.7674, val loss 1.8019\n","step 1800: train loss 1.7695, val loss 1.7935\n","step 1900: train loss 1.7485, val loss 1.7786\n","step 2000: train loss 1.7589, val loss 1.7718\n","step 2100: train loss 1.7398, val loss 1.7733\n","step 2200: train loss 1.7126, val loss 1.7466\n","step 2300: train loss 1.7071, val loss 1.7334\n","step 2400: train loss 1.6975, val loss 1.7260\n","step 2500: train loss 1.7071, val loss 1.7189\n","step 2600: train loss 1.6827, val loss 1.7121\n","step 2700: train loss 1.6767, val loss 1.7039\n","step 2800: train loss 1.6680, val loss 1.6912\n","step 2900: train loss 1.6608, val loss 1.6912\n","step 3000: train loss 1.6538, val loss 1.6855\n","step 3100: train loss 1.6603, val loss 1.6838\n","step 3200: train loss 1.6377, val loss 1.6727\n","step 3300: train loss 1.6422, val loss 1.6602\n","step 3400: train loss 1.6251, val loss 1.6554\n","step 3500: train loss 1.6387, val loss 1.6508\n","step 3600: train loss 1.6186, val loss 1.6362\n","step 3700: train loss 1.6071, val loss 1.6405\n","step 3800: train loss 1.6064, val loss 1.6373\n","step 3900: train loss 1.5865, val loss 1.6288\n","step 4000: train loss 1.5993, val loss 1.6246\n","step 4100: train loss 1.5859, val loss 1.6124\n","step 4200: train loss 1.5802, val loss 1.6032\n","step 4300: train loss 1.5838, val loss 1.6051\n","step 4400: train loss 1.5635, val loss 1.6079\n","step 4500: train loss 1.5693, val loss 1.5927\n","step 4600: train loss 1.5690, val loss 1.6001\n","step 4700: train loss 1.5664, val loss 1.5890\n","step 4800: train loss 1.5634, val loss 1.5896\n","step 4900: train loss 1.5591, val loss 1.5854\n","step 4999: train loss 1.5616, val loss 1.5842\n","\n","\n","     \"What kning\n","     on I am to resore eye cams\n","     to 'nexter feelock of her in the pizle, Watson not the clooke in we to same\n","     to\n","\n","                  \"When we was befoen the briend spoking of\n","     linkic which hands to pain atties. I port quintel b?\" asked, the thinking there round which closs way or you.\"\n","\n","     \"You absor's, be studder, you throu palated you\n","     surpid, he irenclisious.\n","\n","     \"You aresfair the old mew was in as in the some ding a\n","     mat from do farw in.\n","\n","     \"I could the dishting bitle of a had It am at as be duck and he shen?\"\n","\n","     \"He dridchy and\n","     the cruling\n","     brom as eflecter.\"\n","\n","     \"You are forest to he well, How lasce came was a his\n","     line McGissor, stuch to peily it. There instence-to poor yould of aman us.\n","\n","     \"Well, with show it roold, and I still conto of can atclres,\n","     the table,\n","     Luchical as on frossion without the onturning bronsh sking only and surithin me frulling on his fellowent stime spret sir.\"\n","\n","     \"There which tome irousp when I flow!\n","\n","     \"You hims?\"\n","\n","     \"No; Gridguine and he wilkejor from inut, Should risor in the rad is. \"Wellower.\n","\n","     They all singrience of sowle at young so manickingy by thoughtinger of the impsidess. How are him\n","     hoper very and\n","     thran one,\" said I accyks to han is ansoce followish well the door o us could go news gulcame out there osclock of the inqurnie of in all in the glain. Do you we oup no put at house interistance Sinclinamince.\n","\n","     \"Then I dish\n","     down three othin\n","     the wason'y gotgleshe of purith our broing up the angated\n","     eveniness, the pcallared whose hould lip for Mandogers,\" said he, all went\n","     uTurning of the absen strack Lestant a loove of the sarpored.\n","\n","     \"Oh! let it from Holmes were in in his sluch erening foever the old\n","     his all agreatizir turnusious.\n","     In onear-legge. A deighter to fiver down\n","     over\n","     by saw and\n","     you me. He appracy dover.\"\n","\n","     \"I have he had\n","     she pot's\n","     my sir, and and the distract\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Tw2tYBoURfQq"},"execution_count":null,"outputs":[]}]}